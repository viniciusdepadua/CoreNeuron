include:
  - project: hpc/gitlab-pipelines
    file:
      - spack-build-components.gitlab-ci.yml
      - github-project-pipelines.gitlab-ci.yml

stages:
    - .pre
    - build
    - test
    - build_neuron
    - test_neuron

# Set up Spack
spack_setup:
  extends: .spack_setup_ccache
  variables:
    # Enable fetching GitHub PR descriptions and parsing them to find out what
    # branches to build of other projects.
    PARSE_GITHUB_PR_DESCRIPTIONS: "true"
  script:
    - !reference [.spack_setup_ccache, script]
    # This allows us to use the CoreNEURON repository in regression tests of
    # the gitlab-pipelines repositories. The regression test pipeline triggers
    # *this* pipeline as a child, having pushed a modified branch to the GitLab
    # mirror of the CoreNEURON repository. We have to update the Spack recipe
    # to point at the GitLab mirror so the relevant commit (on the modified
    # branch) can be found.
    - if [[ "${CI_PIPELINE_SOURCE}" == "pipeline" ]]; then
    - cd $(spack location -p coreneuron)
    - sed -i -e 's#\(git\s*=\s\)"https://github.com/BlueBrain/CoreNeuron"#\1"git@bbpgitlab.epfl.ch:hpc/coreneuron.git"#' package.py
    - git diff
    - fi

.spack_intel:
  variables:
    SPACK_PACKAGE_COMPILER: intel
.spack_nvhpc:
  variables:
    # We have to run GPU builds on GPU nodes for driver/makelocalrc reasons.
    # Because there is an outstanding issue with HTTPS downloads on the phase 2
    # nodes (HELP-13868), which makes Spack build jobs fail there, we restrict
    # to phase 1 nodes for building.
    bb5_constraint: 'volta&rack2'
    SPACK_PACKAGE_COMPILER: nvhpc
.spack_neuron:
  variables:
    SPACK_PACKAGE: neuron
    SPACK_PACKAGE_REF: ''
    SPACK_PACKAGE_SPEC: +coreneuron+debug+tests~legacy-unit

build:coreneuron:intel:
  variables:
    SPACK_PACKAGE: coreneuron
    SPACK_PACKAGE_SPEC: +tests~legacy-unit build_type=Debug
  extends:
    - .spack_build
    - .spack_intel

build:coreneuron:gpu:
  variables:
    SPACK_PACKAGE: coreneuron
    # +report pulls in a lot of dependencies and the tests fail.
    # See https://github.com/BlueBrain/CoreNeuron/issues/518 re: build_type
    SPACK_PACKAGE_SPEC: +gpu+tests~legacy-unit~report build_type=RelWithDebInfo
  extends:
    - .spack_build
    - .spack_nvhpc

test:coreneuron:intel:
  extends: [.ctest]
  needs: ["build:coreneuron:intel"]

test:coreneuron:gpu:
  extends:
    - .ctest
  variables:
    # GPU tests need to run on nodes with GPUs.
    bb5_constraint: volta
  needs: ["build:coreneuron:gpu"]

build:neuron:intel:
  stage: build_neuron
  extends:
    - .spack_build
    - .spack_neuron
    - .spack_intel
  needs: ["build:coreneuron:intel"]

build:neuron:gpu:
  stage: build_neuron
  extends:
    - .spack_build
    - .spack_neuron
    - .spack_nvhpc
  variables:
    # Avoid trying to build all the NEURON dependencies with the PGI/NVHPC/GPU
    # compiler.
    SPACK_EXPORT_SPECS: gettext
  before_script:
    # Get the hash of a gcc build of py-cython so we can force the nvhpc build
    # of neuron to use it. py-cython does not build with nvhpc.
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
    - CYTHON_HASH=$(spack find --json py-cython%gcc | python -c "import json, sys; print(json.loads(sys.stdin.read())[0]['hash'])")
    # Inject the hash for py-cython after the one for coreneuron; also say
    # numpy has to be built with GCC to minimise the amount of time we spend
    # building dependencies in the CI.
    - SPACK_PACKAGE_DEPENDENCIES="${SPACK_PACKAGE_DEPENDENCIES}^/${CYTHON_HASH}^py-numpy%gcc"
    - !reference [.spack_build, before_script]
  needs: ["build:coreneuron:gpu"]

test:neuron:intel:
  stage: test_neuron
  extends: [.ctest]
  needs: ["build:neuron:intel"]

test:neuron:gpu:
  stage: test_neuron
  extends: [.ctest]
  variables:
    # GPU tests need to run on nodes with GPUs.
    bb5_constraint: volta
  needs: ["build:neuron:gpu"]
